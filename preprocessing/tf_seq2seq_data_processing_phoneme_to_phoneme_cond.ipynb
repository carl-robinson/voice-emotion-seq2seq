{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Process Multiple Files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make F0 raw csv files per phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# time step is 0.005 seconds, but the syll time resolution is 4dp, so must round\n",
    "\n",
    "# for each mat file:\n",
    "#     load mat file\n",
    "#     extract f0_raw, syll_label and syll_time into variables, and reshape\n",
    "#     for each syll_label\n",
    "#         if syll_label contains a '#' in it\n",
    "#             get syll_time start and end times\n",
    "#             divide each by 0.005 to get start and end indexes\n",
    "#             extract f0_raw range using indexes\n",
    "#             do the f0 rounding, shifting, casting as before\n",
    "#             write to file, adding syllable loop index to end of filename\n",
    "#         else\n",
    "#             skip to next syllable\n",
    "\n",
    "# must also modify combo source/target section that follows, to remove segmenting parts\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "# set number of decimal places\n",
    "dec = 0\n",
    "\n",
    "# path to input files directory\n",
    "directory_path_root = '/Users/robinson/Dropbox/anasynth/_data/emoVC/Olivia2006'\n",
    "directory = os.fsencode(directory_path_root)\n",
    "# path to output files directory\n",
    "directory_path_f0raw = os.path.join(directory_path_root, 'f0_raw_phoneme')\n",
    "if not os.path.exists(directory_path_f0raw):\n",
    "    os.mkdir(directory_path_f0raw)\n",
    "\n",
    "    \n",
    "# lists to store all syllables and phonemes in all files\n",
    "all_phonemes = []\n",
    "all_syllables = []\n",
    "\n",
    "# list of unvoiced phoneme labels\n",
    "vowel_phonemes = ['e~', '9~', 'a~', 'o~', 'i', 'e', 'E', 'a', 'A', 'O', 'o', 'u', 'y', '2', '9', '@']\n",
    "\n",
    "# prefix to add to f0 values based on syllable position in phrase\n",
    "prefix = ''\n",
    "\n",
    "def max_round_save(i, k, f0_accum_list, phone_label_string):\n",
    "    # impose max f0 value\n",
    "    max_f0 = 550\n",
    "    # voiced = [True if '#' not in syll[0] else False for i, syll in enumerate(syll_label)]\\n\",\n",
    "    f0_accum_list = [x if x < max_f0 else max_f0 for x in f0_accum_list]\n",
    "    f0_accum_array = np.array(f0_accum_list)\n",
    "    \n",
    "    # create new array to hold rounded values\n",
    "    phone_f0_dec = np.zeros(f0_accum_array.shape)\n",
    "    # round all values to dec dp\n",
    "    np.around(f0_accum_array, decimals=dec, out=phone_f0_dec)\n",
    "    # multiply by 10^dec to shift dp dec places to the right\n",
    "    phone_f0_dec = phone_f0_dec * (10**dec)\n",
    "    # cast to int to ensure precise number representation in memory\n",
    "    phone_f0_dec = phone_f0_dec.astype(int)\n",
    "    \n",
    "    # comment out for no con\n",
    "    phone_f0_dec_prefixed = []\n",
    "    [phone_f0_dec_prefixed.append(prefix + str(x)) for x in phone_f0_dec]\n",
    "    \n",
    "    # write out csv file of f0_raw values - specify format as %u for values to be written as int\n",
    "    # add syllable loop index to end of filename\n",
    "    filename_noext, _ = os.path.splitext(filename)\n",
    "    output_file_extension = '.csv'\n",
    "    # removing the syllable marker, as the same phoneme may not belong to the same syllable\n",
    "    # output_file_name = ''.join([filename_noext, '.s', format(i, '02d'), \n",
    "    #                             '_', phone_label_string, str(k), output_file_extension])    \n",
    "    output_file_name = ''.join([filename_noext, '_', phone_label_string, str(k), output_file_extension])\n",
    "    # print('output_file_name: ', output_file_name)\n",
    "    np.savetxt(os.path.join(directory_path_f0raw, output_file_name), phone_f0_dec_prefixed, delimiter=',', fmt='%s')                         \n",
    "\n",
    "\n",
    "# for each mat file in directory (each mat file has one sequence of f0 raw values in it)\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.mat'): \n",
    "        # build filepath (should use file var here really)\n",
    "        filepath = os.path.join(directory_path_root, filename)\n",
    "        # print(filepath)\n",
    "        \n",
    "        # load the file and extract f0 raw, syll_label and syll_time into variables, and reshape\n",
    "        mat_dict = loadmat(filepath)\n",
    "        f0_raw = mat_dict['f0_raw']\n",
    "        f0_raw = f0_raw.reshape((f0_raw.shape[1],))\n",
    "        \n",
    "        syll_label = mat_dict['syll_label']\n",
    "        syll_label = syll_label.reshape((syll_label.shape[1],))        \n",
    "        # print(syll_label.shape)\n",
    "        # print(syll_label)\n",
    "\n",
    "#         for label in syll_label:\n",
    "#             print(label[0])\n",
    "        \n",
    "        # reshape this to 2d, to preserve start/end relationship\n",
    "        # syll_time.shape (2, 11)\n",
    "        # I want syll_time.shape (11, 2) BUT with the start and end times in different 'columns' - just transpose!\n",
    "        syll_time = mat_dict['syll_time']\n",
    "        # print('syll_time.shape', syll_time.shape)\n",
    "        syll_time = syll_time.T\n",
    "        # print('syll_time.shape', syll_time.shape)\n",
    "        # print(syll_time)\n",
    "        \n",
    "        first_nonhash_index = 999\n",
    "        last_nonhash_index = 999\n",
    "        \n",
    "        # get first non-hash syllable index\n",
    "        for i, syll in enumerate(syll_label):\n",
    "            if '#' not in syll[0]:\n",
    "                first_nonhash_index = i\n",
    "                break\n",
    "        # print('first_nonhash_index', first_nonhash_index)\n",
    "        \n",
    "        # get last non-hash syllable index\n",
    "        indicies = reversed(range(len(syll_label)))\n",
    "        for i, syll in zip(indicies, syll_label):\n",
    "            if '#' not in syll[0]:\n",
    "                last_nonhash_index = i\n",
    "                break\n",
    "        # print('last_nonhash_index', last_nonhash_index)\n",
    "                \n",
    "        # get phoneme labels and time\n",
    "        phone_label = mat_dict['phone_label']\n",
    "        phone_label = phone_label.reshape((phone_label.shape[1],))\n",
    "        # [print('phone_label ' + str(x) + ':', phone_label[x][0]) for x in range(phone_label.shape[0])]\n",
    "        phone_time = mat_dict['phone_time']\n",
    "        phone_time = phone_time.T\n",
    "\n",
    "        # break #debug\n",
    "        \n",
    "        # for each syll\n",
    "        # if # skip\n",
    "        # else get list of phonemes in that syllable - how? the phone time and syll times align perfectly, so search        \n",
    "        # iterate through phonemes\n",
    "        #     if unvoiced reached (lookup in list of unvoiced)\n",
    "        #           if f0 list not empty\n",
    "        #               impose max/round/write out, using accumulated phone labels as filename\n",
    "        #               reset phone_label_list\n",
    "        #           continue iteration\n",
    "        #     else voiced\n",
    "        #           get start/end times, calc indices and extract f0 for it, add to accumulating list (also add \n",
    "        #           phoneme label to string)\n",
    "        #     if end of list reached (use else block on loop)\n",
    "        #           impose max/round/write out, and break to next syllable\n",
    "        #           reset phone_label_list\n",
    "        \n",
    "        # i'll need to modify f0_transform too, to calc the start/end times using the durations of the phonemes in \n",
    "        # the mat file \n",
    "        \n",
    "        # dict to hold phoneme identifiers for this file (phrase)\n",
    "        phon_dict = {}        \n",
    "\n",
    "        # for each syll in syll_label\n",
    "        for i, syll in enumerate(syll_label):\n",
    "\n",
    "            # if syll_label doesn't contain a '#' in it\n",
    "            if '#' not in syll[0]:\n",
    "                # print(syll[0])\n",
    "                \n",
    "                # if first syllable, prefix with 'a'\n",
    "                if i == first_nonhash_index:\n",
    "                    prefix = 'a'\n",
    "                # if last syllable, prefix with 'c'\n",
    "                elif i == last_nonhash_index:\n",
    "                    prefix = 'c'\n",
    "                # if anything else, it's a middle syllable, prefix with 'b'\n",
    "                else:\n",
    "                    prefix = 'b'\n",
    "                \n",
    "\n",
    "                # add syllable to a list\n",
    "                all_syllables.append(syll[0] + str(i))\n",
    "\n",
    "                # get syll_time start and end times\n",
    "                # print('i = ', i)\n",
    "                syll_start_time = syll_time[i, 0]\n",
    "                syll_end_time = syll_time[i, 1]\n",
    "                \n",
    "                phone_id_list = []\n",
    "                # for each phone label in phone_label (this mat file)\n",
    "                for j, label in enumerate(phone_label):\n",
    "                    # get start/end times from phone_time\n",
    "                    phone_start_time = phone_time[j, 0]\n",
    "                    phone_end_time = phone_time[j, 1]\n",
    "                    # if start time => syll_start_time AND end time <= syll_end_time then this phoneme is in the syllable\n",
    "                    if phone_start_time >= syll_start_time and phone_end_time <= syll_end_time:\n",
    "                        # add phoneme label id to a list (to use to reference label and start/end times)\n",
    "                        phone_id_list.append(j)\n",
    "                \n",
    "                # print('phone_id_list ', phone_id_list)\n",
    "                \n",
    "                # iterate through phonemes of this syllable\n",
    "                for k in phone_id_list:\n",
    "                    # make dictionary key for phrase/phoneme combo\n",
    "                    # keys and vals can be of any type\n",
    "                    # use 'in' to check if a key is in a dict (but not a value)\n",
    "                    \n",
    "                    # Do a fresh dict for each mat file. For each phoneme, add the phon_label to the dict as a key \n",
    "                    # with value <phon_label>_1 (f for phoneme). if it already exists, increment the value to \n",
    "                    # 2.. Use this value as the postfix on the filename. As long as there are the same number of \n",
    "                    # phonemes in each file, regardless of their position, they will match up during the \n",
    "                    # intersection.   \n",
    "                    \n",
    "                    # check if phone_label[k][0] is in phon_dict\n",
    "                    # if so, incremement the value and save value into phon_incval\n",
    "                    if phone_label[k][0] in phon_dict:\n",
    "                        phon_dict[phone_label[k][0]] = phon_dict[phone_label[k][0]] + 1\n",
    "                        phon_incval = phon_dict[phone_label[k][0]]\n",
    "                        # print('IN')\n",
    "                    # if not, add it with value 1 and save value into phon_incval\n",
    "                    else:    \n",
    "                        phon_dict[phone_label[k][0]] = 1\n",
    "                        phon_incval = phon_dict[phone_label[k][0]]\n",
    "                        # print('NOT')\n",
    "                    # append the phone_label[k][0] + phon_incval to all_phonemes \n",
    "                    # print('phone_label[k][0] = ', phone_label[k][0])\n",
    "                    # print('phon_label = {}'.format(phone_label[k][0] + str(phon_incval)))\n",
    "                    all_phonemes.append(phone_label[k][0] + str(phon_incval))\n",
    "                    # all_phonemes.append(phone_label[k][0] + str(k))\n",
    "                    # print('all_phonemes.append : ', phone_label[k][0] + str(k))\n",
    "                    \n",
    "                    # reset vars\n",
    "                    f0_accum_list = []       \n",
    "                    # if unvoiced reached (lookup in list of unvoiced) if f0 list not empty..\n",
    "                    # i.e. if you hit a non-vowel but there's nothing in f0_accum_list, then just skip ahead\n",
    "                    # if phone_label[k][0] not in vowel_phonemes and f0_accum_list:\n",
    "                    #     print('IN NON-VOWEL PHONEME')\n",
    "                    #     print('k : ', k)\n",
    "                    #     print('non-vowel: ', phone_label[k][0])\n",
    "                    #     # impose max/round/write out, using accumulated phone labels\n",
    "                    #     # MUST use k-1 as we're on the NEXT phoneme, but want to write prev\n",
    "                    #     # e.g. s O l, on the l but write O\n",
    "                    #     print('writing previous phoneme... ', phone_label[k-1][0])\n",
    "                    #     max_round_save(i, k - 1)\n",
    "                    if phone_label[k][0] not in vowel_phonemes:\n",
    "                        # do nothing\n",
    "                        # print('IN NON-VOWEL PHONEME (f0 list empty)')\n",
    "                        # print('k : ', k)\n",
    "                        # print('non-vowel: ', phone_label[k][0])\n",
    "                        continue\n",
    "                    # vowel, so accumulate f0 values\n",
    "                    else:\n",
    "                        # print('IN VOWEL PHONEME')\n",
    "                        # print('k : ', k)\n",
    "                        # print('vowel: ', phone_label[k][0])\n",
    "                        # get start/end times, calc indices and extract f0 for it\n",
    "                        phone_start_time = phone_time[k, 0]\n",
    "                        phone_end_time = phone_time[k, 1]                        \n",
    "\n",
    "                        # divide each by 0.005 to get start and end indexes\n",
    "                        phone_start_idx = (int)(phone_start_time // 0.005)\n",
    "                        phone_end_idx = (int)(phone_end_time // 0.005)\n",
    "                        # print(phone_start_idx)\n",
    "                        # print(phone_end_idx)\n",
    "        \n",
    "                        # extract f0_raw range using indexes\n",
    "                        phone_f0 = f0_raw[phone_start_idx:phone_end_idx]\n",
    "                        # print('phone_f0 ', phone_f0)\n",
    "                        \n",
    "                        # add to accumulating list\n",
    "                        f0_accum_list = list(phone_f0)\n",
    "\n",
    "                        phone_label_string = phone_label[k][0]\n",
    "                        \n",
    "                        # print('phone_label_string', phone_label_string)\n",
    "                        \n",
    "                        max_round_save(i, phon_incval, f0_accum_list, phone_label_string)\n",
    "                        \n",
    "                # else:\n",
    "                #     print('IN FINAL ELSE - NO MORE PHONEMES IN SYLL')\n",
    "                #     # if there are remaining values in f0 list\n",
    "                #     if f0_accum_list:\n",
    "                #         print('k OUT of for: ', k)\n",
    "                #         # impose max/round/write out, using accumulated phone labels as\n",
    "                #         max_round_save(i, k)\n",
    "#                 break #debug\n",
    "\n",
    "            # syll_label contains a '#' in it (an unvoiced region), skip to next syllable\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # break  # debug - one iteration only\n",
    "        \n",
    "    # file is not .mat, so skip\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Make Combo Source and Target Syllable Input Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# the above code makes output files with one row per syllable\n",
    "# we can't use these as-is, as we need to read the lines in as pairs, so source and target must have equal num of rows\n",
    "# next step is to pair the files using the phrase and intensities in the filenames\n",
    "\n",
    "# source: 10 phrases of i00 intensity across e01 to e08 - each phrase is said 8 times, neutrally\n",
    "# target: 10 phrases of i01-i05 intensity for e02 - each phrase is said 5 times, expressively (5 times)\n",
    "# so for each utterance (8 of) of each 'p' source phrase (10 of), copy it 5 times, matched with i01-i05 of 'p' target\n",
    "# P(10) > E(8) > I(5)\n",
    "\n",
    "# build paths and open output files\n",
    "# path to input files directories\n",
    "input_directory_path = '/Users/robinson/Dropbox/anasynth/_data/emoVC/Olivia2006/f0_raw_phoneme'\n",
    "# define filename components\n",
    "# Olivia2006.e02.p01.i01.csv\n",
    "input_file_root = 'Olivia2006'\n",
    "input_file_extension = '.csv'\n",
    "\n",
    "# define output filenames and paths\n",
    "output_directory = os.path.join(input_directory_path, 'out')\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "# output filenames\n",
    "filename_source = 'source.txt'\n",
    "filename_target = 'target.txt'\n",
    "filename_log = 'log.txt'\n",
    "# open output files in subdirectory of input files directory (must create manually)\n",
    "fs = open(os.path.join(output_directory, filename_source), 'w')\n",
    "ft = open(os.path.join(output_directory, filename_target), 'w')\n",
    "fo = open(os.path.join(output_directory, filename_log), 'w')\n",
    "\n",
    "\n",
    "# pass it a symbol string 'p' / 'e' / 'i' with range, or a phoneme code string\n",
    "# it finds all files in a directory that have this in their filename, and returns their filenames as a set\n",
    "def getSet(symbol, num_from=None, num_to=None):\n",
    "    # path to input files directory\n",
    "    directory = os.fsencode(input_directory_path)\n",
    "    \n",
    "    # filepath_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    # for each csv file in directory (each csv file has one sequence of f0 raw values in it)\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith('.csv'): \n",
    "            # build filepath (should use file var here really)\n",
    "            # filepath = os.path.join(input_directory_path, filename)\n",
    "            \n",
    "            # if num_from is set, then it's either a p/e/i, so loop the range specified\n",
    "            if num_from != None:\n",
    "                for i in range(num_from, num_to + 1):\n",
    "                    if ''.join(['.', symbol, format(i, '02d')]) in filename:\n",
    "                        filename_list.append(filename)\n",
    "            # if num_from is not set, then it's a phoneme symbol specified\n",
    "            else:\n",
    "                if ''.join(['_', symbol, '.']) in filename:\n",
    "                        filename_list.append(filename)\n",
    "\n",
    "    # return a set of unique filenames that satisfy the given parameters\n",
    "    return set(filename_list)\n",
    "\n",
    "\n",
    "# #####################\n",
    "# DEFINE PARAMETERS\n",
    "\n",
    "# 1. Joie douce\n",
    "# 2. Joie explosive\n",
    "# 3. Peur contenue\n",
    "# 4. Peur hystérique\n",
    "# 5. Tristesse contenue\n",
    "# 6. Tristesse larmoyante\n",
    "# 7. Colère contenue\n",
    "# 8. Colère explosive\n",
    "\n",
    "# define phrase range\n",
    "phrase_from = 1\n",
    "phrase_to = 10\n",
    "# phrase_list = [1, 3, 5, 10]\n",
    "phrase_list = [2, 4, 6, 7, 8, 9]\n",
    "# define source and target emotion ranges\n",
    "source_emotion_from = 1\n",
    "source_emotion_to = 8\n",
    "target_emotion_from = 8  # 2 for joie explosif, 8 for colere explosif\n",
    "target_emotion_to = 8\n",
    "# define source and target intensity ranges\n",
    "source_intensity_from = 0\n",
    "source_intensity_to = 0\n",
    "target_intensity_from = 1\n",
    "target_intensity_to = 5\n",
    "\n",
    "# END PARAMETERS\n",
    "# #######################\n",
    "\n",
    "# SOURCE\n",
    "# create lists of sets for each phrase, emotion, intensity and phoneme code\n",
    "set_source_emotions = getSet('e', source_emotion_from, source_emotion_to)\n",
    "set_target_emotions = getSet('e', target_emotion_from, target_emotion_to)\n",
    "set_source_intensities = getSet('i', source_intensity_from, source_intensity_to)\n",
    "set_target_intensities = getSet('i', target_intensity_from, target_intensity_to)\n",
    "\n",
    "# print(set_source_intensities)\n",
    "\n",
    "# that do too, then I just  - do this for all syllables for each syllable, get set of source filenames \n",
    "# which satisfy the parameters, and a set of target filenames that do too, then I just make a set of filename pairs \n",
    "# with a loop (for each filename in source set, match with a filename in target set) - do this for all syllables\n",
    "\n",
    "# get unique list of syllables\n",
    "all_phonemes_set = set([x for x in all_phonemes])\n",
    "# print(len(set_one_syllable))\n",
    "# print(all_syllables_set)\n",
    "\n",
    "# NEW pseudo code\n",
    "# for each phrase in specified phrase range\n",
    "# for phrase in range(phrase_from, phrase_to + 1):\n",
    "for phrase in phrase_list:    \n",
    "#     get set for that phrase (e.g. getSet('p',1,1))\n",
    "    set_phrases = getSet('p', phrase, phrase)\n",
    "    # create empty list to store syllables in the phrase\n",
    "    phrase_phonemes = []\n",
    "\n",
    "    # get all syllables in that phrase\n",
    "    # for each syllable\n",
    "    for phon in all_phonemes_set:\n",
    "        # get list of filenames that have this syllable        \n",
    "        set_one_phoneme = getSet(phon)\n",
    "        # if this syllable exists in this set of phrases, add to the list\n",
    "        if set_one_phoneme & set_phrases:\n",
    "            phrase_phonemes.append(phon)\n",
    "    \n",
    "    # make a set to remove duplicates\n",
    "    set_phrase_phonemes = set(phrase_phonemes)\n",
    "\n",
    "    # for each syllable in this phrase\n",
    "    for phon in set_phrase_phonemes:\n",
    "        # get list of filenames that have this phoneme        \n",
    "        set_one_phoneme = getSet(phon)\n",
    "\n",
    "        # get source filenames for that one phrase, one phon, all emotions combo\n",
    "        # - note: returned set can be empty\n",
    "        if set_one_phoneme & set_phrases & set_source_emotions & set_source_intensities:\n",
    "            set_sources = set.intersection(set_one_phoneme, set_phrases, set_source_emotions, set_source_intensities)\n",
    "    #         print(set_sources)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # get target filenames for that one phrase, one phon, all emotions combo\n",
    "        if set_one_phoneme & set_phrases & set_target_emotions & set_target_intensities:\n",
    "            set_targets = set.intersection(set_one_phoneme, set_phrases, set_target_emotions, set_target_intensities)\n",
    "    #         print(set_targets)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # make pairs of all source and target filenames and write out\n",
    "        # make a set of filename pairs (for every filename in source set, match with every filename in target set)\n",
    "        for source_file in set_sources:\n",
    "            for target_file in set_targets:\n",
    "\n",
    "                # build the source file path\n",
    "                source_file_path = os.path.join(input_directory_path, source_file)           \n",
    "                # build the target file path\n",
    "                target_file_path = os.path.join(input_directory_path, target_file)\n",
    "                # if this file doesn't exist, break out of syllable loop and try next one\n",
    "                if not os.path.isfile(target_file_path) or os.stat(target_file_path).st_size == 0 or not \\\n",
    "                        os.path.isfile(source_file_path) or os.stat(source_file_path).st_size == 0:\n",
    "                    break\n",
    "\n",
    "                # load the source file and extract vars\n",
    "                # source_f0_raw = np.loadtxt(source_file_path, dtype=np.dtype('S2'))\n",
    "                source_f0_raw = np.loadtxt(source_file_path, dtype='str')\n",
    "                # print(source_f0_raw) ['b176' 'b178' 'b180' 'b183' 'b185' 'b187' 'b189' 'b190' 'b191' 'b192'\n",
    "                #  'b194' 'b196' 'b199' 'b202' 'b206' 'b209' 'b212' 'b214' 'b216' 'b217'\n",
    "                #  'b217']\n",
    "                # reshape to have two indices, the first being a constant so all values belong to the same 'row'\n",
    "                # source_f0_raw = source_f0_raw.reshape((1, source_f0_raw.shape[0]))\n",
    "                # print(source_f0_raw.shape)\n",
    "                # print(source_f0_raw)\n",
    "                # append it to output file as a new row, with space delimiter between elements, format unsigned int\n",
    "                # print('source_f0_raw.dtype ', source_f0_raw.dtype)\n",
    "                # print('source_f0_raw.dtype[0] ', source_f0_raw[0].dtype)\n",
    "                csvWriterS = csv.writer(fs, delimiter=' ')  # using the csv module to write the file\n",
    "                csvWriterS.writerow(source_f0_raw)                 # write every row in the matrix\n",
    "                # np.savetxt(fs, source_f0_raw, fmt='%s', delimiter=' ')\n",
    "\n",
    "                # load the target file and extract vars\n",
    "                target_f0_raw = np.loadtxt(target_file_path, dtype='str')\n",
    "                # reshape to have two indices, the first being a constant so all values belong to the same 'row'\n",
    "                # target_f0_raw = target_f0_raw.reshape((1, target_f0_raw.shape[0]))                    \n",
    "                # append it to output file as a new row, with space delimiter between elements, format unsigned int\n",
    "                csvWriterT = csv.writer(ft, delimiter=' ')  # using the csv module to write the file\n",
    "                csvWriterT.writerow(target_f0_raw)                 # write every row in the matrix\n",
    "                # np.savetxt(ft, target_f0_raw, fmt='%s', delimiter=' ')\n",
    "\n",
    "                # write input and output file pair to log file\n",
    "                logstring = source_file_path + '   ' + target_file_path\n",
    "                print(logstring, file=fo)\n",
    "                \n",
    "# close the output files\n",
    "fs.close()\n",
    "ft.close()\n",
    "fo.close()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n1875\n1875\n1875\nfs_lines = 2206\ntrain_lines = 1875\nval_lines = 330\ntest_lines = 1\ndone\n"
     ]
    }
   ],
   "source": [
    "# shuffle the source/target pairs and split them out into train/val/test files\n",
    "\n",
    "# set ratios for train/val/test split e.g. 0.6, 0.2, 0.2\n",
    "train_split = 0.85\n",
    "val_split = 0.15\n",
    "test_split = 0.0  # ok for this to be 0.0, but not the others\n",
    "shuffle = False\n",
    "# \n",
    "# # open source and target input files to read from\n",
    "# fs = open(os.path.join(output_directory, filename_source), 'r')\n",
    "# ft = open(os.path.join(output_directory, filename_target), 'r')\n",
    "\n",
    "# get line counts of files (source and target will be the same, so just need to check one of them)\n",
    "with open(os.path.join(output_directory, filename_source)) as f:\n",
    "    f_lines = sum(1 for _ in f)\n",
    "    # set index values for train, val and test\n",
    "    train_lines = int(f_lines // (1 / train_split))\n",
    "    val_lines = int(f_lines // (1 / val_split))\n",
    "    test_lines = f_lines - train_lines - val_lines # whatever is left\n",
    "\n",
    "# double check that source and target have the same number of lines\n",
    "with open(os.path.join(output_directory, filename_target)) as f2:\n",
    "    f_lines2 = sum(1 for _ in f2)\n",
    "    if f_lines != f_lines2:\n",
    "        raise ValueError('Not the same')\n",
    "    \n",
    "# open source and target input files to read from\n",
    "fs = open(os.path.join(output_directory, filename_source), 'r')\n",
    "ft = open(os.path.join(output_directory, filename_target), 'r')\n",
    "\n",
    "# read the source and target input files line by line, stripping all whitespace and empty lines\n",
    "source_data = fs.read().strip().split('\\n')\n",
    "# print(type(source_data))\n",
    "# print(len(source_data)) #6597\n",
    "target_data = ft.read().strip().split('\\n')\n",
    "# print(len(target_data)) #6597\n",
    "\n",
    "# make a list of tuples, each holding a pair of source and target strings\n",
    "merged_data = list(zip(source_data, target_data))\n",
    "# shuffle the tuples (preserving the pairing) to ensure a good mix of p/e/i in each set\n",
    "if shuffle:\n",
    "    random.shuffle(merged_data)\n",
    "# print(len(merged_data)) #6597\n",
    "\n",
    "# seperate the tuples into two lists of source and target lines\n",
    "train_data_source = [x[0] for x in merged_data[:train_lines]]\n",
    "train_data_target = [x[1] for x in merged_data[:train_lines]]\n",
    "val_data_source = [x[0] for x in merged_data[train_lines:(train_lines+val_lines)]]\n",
    "val_data_target = [x[1] for x in merged_data[train_lines:(train_lines+val_lines)]]\n",
    "test_data_source = [x[0] for x in merged_data[(train_lines+val_lines):]]\n",
    "test_data_target = [x[1] for x in merged_data[(train_lines+val_lines):]]\n",
    "\n",
    "print(len(train_data_source))\n",
    "print(len(train_data_target))\n",
    "# print(len(val_data_source))\n",
    "# print(len(val_data_target))\n",
    "# print(len(test_data_source))\n",
    "# print(len(test_data_target))\n",
    "\n",
    "# make train, test, dev, model directories\n",
    "train_dir = os.path.join(output_directory, 'train')\n",
    "dev_dir = os.path.join(output_directory, 'dev')\n",
    "test_dir = os.path.join(output_directory, 'test')\n",
    "model_dir = os.path.join(output_directory, 'model')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "if not os.path.exists(dev_dir):\n",
    "    os.mkdir(dev_dir)\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "# open output files to write to\n",
    "f_train_source = open(os.path.join(train_dir, 'train_source.txt'), 'w')\n",
    "f_train_target = open(os.path.join(train_dir, 'train_target.txt'), 'w')\n",
    "f_val_source = open(os.path.join(dev_dir, 'val_source.txt'), 'w')\n",
    "f_val_target = open(os.path.join(dev_dir, 'val_target.txt'), 'w')\n",
    "f_test_source = open(os.path.join(test_dir, 'test_source.txt'), 'w')\n",
    "f_test_target = open(os.path.join(test_dir, 'test_target.txt'), 'w')\n",
    "\n",
    "# print(train_data_source)\n",
    "\n",
    "# write each of the lists to the opened files\n",
    "print(len([line for line in train_data_source]))\n",
    "print(len([line for line in train_data_target]))\n",
    "\n",
    "[print(line, file=f_train_source) for line in train_data_source]\n",
    "[print(line, file=f_train_target) for line in train_data_target]\n",
    "[print(line, file=f_val_source) for line in val_data_source]\n",
    "[print(line, file=f_val_target) for line in val_data_target]\n",
    "[print(line, file=f_test_source) for line in test_data_source]\n",
    "[print(line, file=f_test_target) for line in test_data_target]\n",
    "\n",
    "# close the input source and target files\n",
    "fs.close()\n",
    "ft.close()\n",
    "\n",
    "# close the output files\n",
    "f_train_source.close()\n",
    "f_train_target.close()\n",
    "f_val_source.close()\n",
    "f_val_target.close()\n",
    "f_test_source.close()\n",
    "f_test_target.close()\n",
    "\n",
    "print('fs_lines = ' + str(f_lines))\n",
    "print('train_lines = ' + str(train_lines))\n",
    "print('val_lines = ' + str(val_lines))\n",
    "print('test_lines = ' + str(test_lines))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Make Vocabulary Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a50', 'a51', 'a52', 'a53', 'a54', 'a55', 'a56', 'a57', 'a58', 'a59', 'a60', 'a61', 'a62', 'a63', 'a64', 'a65', 'a66', 'a67', 'a68', 'a69', 'a70', 'a71', 'a72', 'a73', 'a74', 'a75', 'a76', 'a77', 'a78', 'a79', 'a80', 'a81', 'a82', 'a83', 'a84', 'a85', 'a86', 'a87', 'a88', 'a89', 'a90', 'a91', 'a92', 'a93', 'a94', 'a95', 'a96', 'a97', 'a98', 'a99', 'a100', 'a101', 'a102', 'a103', 'a104', 'a105', 'a106', 'a107', 'a108', 'a109', 'a110', 'a111', 'a112', 'a113', 'a114', 'a115', 'a116', 'a117', 'a118', 'a119', 'a120', 'a121', 'a122', 'a123', 'a124', 'a125', 'a126', 'a127', 'a128', 'a129', 'a130', 'a131', 'a132', 'a133', 'a134', 'a135', 'a136', 'a137', 'a138', 'a139', 'a140', 'a141', 'a142', 'a143', 'a144', 'a145', 'a146', 'a147', 'a148', 'a149', 'a150', 'a151', 'a152', 'a153', 'a154', 'a155', 'a156', 'a157', 'a158', 'a159', 'a160', 'a161', 'a162', 'a163', 'a164', 'a165', 'a166', 'a167', 'a168', 'a169', 'a170', 'a171', 'a172', 'a173', 'a174', 'a175', 'a176', 'a177', 'a178', 'a179', 'a180', 'a181', 'a182', 'a183', 'a184', 'a185', 'a186', 'a187', 'a188', 'a189', 'a190', 'a191', 'a192', 'a193', 'a194', 'a195', 'a196', 'a197', 'a198', 'a199', 'a200', 'a201', 'a202', 'a203', 'a204', 'a205', 'a206', 'a207', 'a208', 'a209', 'a210', 'a211', 'a212', 'a213', 'a214', 'a215', 'a216', 'a217', 'a218', 'a219', 'a220', 'a221', 'a222', 'a223', 'a224', 'a225', 'a226', 'a227', 'a228', 'a229', 'a230', 'a231', 'a232', 'a233', 'a234', 'a235', 'a236', 'a237', 'a238', 'a239', 'a240', 'a241', 'a242', 'a243', 'a244', 'a245', 'a246', 'a247', 'a248', 'a249', 'a250', 'a251', 'a252', 'a253', 'a254', 'a255', 'a256', 'a257', 'a258', 'a259', 'a260', 'a261', 'a262', 'a263', 'a264', 'a265', 'a266', 'a267', 'a268', 'a269', 'a270', 'a271', 'a272', 'a273', 'a274', 'a275', 'a276', 'a277', 'a278', 'a279', 'a280', 'a281', 'a282', 'a283', 'a284', 'a285', 'a286', 'a287', 'a288', 'a289', 'a290', 'a291', 'a292', 'a293', 'a294', 'a295', 'a296', 'a297', 'a298', 'a299', 'a300', 'a301', 'a302', 'a303', 'a304', 'a305', 'a306', 'a307', 'a308', 'a309', 'a310', 'a311', 'a312', 'a313', 'a314', 'a315', 'a316', 'a317', 'a318', 'a319', 'a320', 'a321', 'a322', 'a323', 'a324', 'a325', 'a326', 'a327', 'a328', 'a329', 'a330', 'a331', 'a332', 'a333', 'a334', 'a335', 'a336', 'a337', 'a338', 'a339', 'a340', 'a341', 'a342', 'a343', 'a344', 'a345', 'a346', 'a347', 'a348', 'a349', 'a350', 'a351', 'a352', 'a353', 'a354', 'a355', 'a356', 'a357', 'a358', 'a359', 'a360', 'a361', 'a362', 'a363', 'a364', 'a365', 'a366', 'a367', 'a368', 'a369', 'a370', 'a371', 'a372', 'a373', 'a374', 'a375', 'a376', 'a377', 'a378', 'a379', 'a380', 'a381', 'a382', 'a383', 'a384', 'a385', 'a386', 'a387', 'a388', 'a389', 'a390', 'a391', 'a392', 'a393', 'a394', 'a395', 'a396', 'a397', 'a398', 'a399', 'a400', 'a401', 'a402', 'a403', 'a404', 'a405', 'a406', 'a407', 'a408', 'a409', 'a410', 'a411', 'a412', 'a413', 'a414', 'a415', 'a416', 'a417', 'a418', 'a419', 'a420', 'a421', 'a422', 'a423', 'a424', 'a425', 'a426', 'a427', 'a428', 'a429', 'a430', 'a431', 'a432', 'a433', 'a434', 'a435', 'a436', 'a437', 'a438', 'a439', 'a440', 'a441', 'a442', 'a443', 'a444', 'a445', 'a446', 'a447', 'a448', 'a449', 'a450', 'a451', 'a452', 'a453', 'a454', 'a455', 'a456', 'a457', 'a458', 'a459', 'a460', 'a461', 'a462', 'a463', 'a464', 'a465', 'a466', 'a467', 'a468', 'a469', 'a470', 'a471', 'a472', 'a473', 'a474', 'a475', 'a476', 'a477', 'a478', 'a479', 'a480', 'a481', 'a482', 'a483', 'a484', 'a485', 'a486', 'a487', 'a488', 'a489', 'a490', 'a491', 'a492', 'a493', 'a494', 'a495', 'a496', 'a497', 'a498', 'a499', 'a500', 'a501', 'a502', 'a503', 'a504', 'a505', 'a506', 'a507', 'a508', 'a509', 'a510', 'a511', 'a512', 'a513', 'a514', 'a515', 'a516', 'a517', 'a518', 'a519', 'a520', 'a521', 'a522', 'a523', 'a524', 'a525', 'a526', 'a527', 'a528', 'a529', 'a530', 'a531', 'a532', 'a533', 'a534', 'a535', 'a536', 'a537', 'a538', 'a539', 'a540', 'a541', 'a542', 'a543', 'a544', 'a545', 'a546', 'a547', 'a548', 'a549', 'a550', 'b50', 'b51', 'b52', 'b53', 'b54', 'b55', 'b56', 'b57', 'b58', 'b59', 'b60', 'b61', 'b62', 'b63', 'b64', 'b65', 'b66', 'b67', 'b68', 'b69', 'b70', 'b71', 'b72', 'b73', 'b74', 'b75', 'b76', 'b77', 'b78', 'b79', 'b80', 'b81', 'b82', 'b83', 'b84', 'b85', 'b86', 'b87', 'b88', 'b89', 'b90', 'b91', 'b92', 'b93', 'b94', 'b95', 'b96', 'b97', 'b98', 'b99', 'b100', 'b101', 'b102', 'b103', 'b104', 'b105', 'b106', 'b107', 'b108', 'b109', 'b110', 'b111', 'b112', 'b113', 'b114', 'b115', 'b116', 'b117', 'b118', 'b119', 'b120', 'b121', 'b122', 'b123', 'b124', 'b125', 'b126', 'b127', 'b128', 'b129', 'b130', 'b131', 'b132', 'b133', 'b134', 'b135', 'b136', 'b137', 'b138', 'b139', 'b140', 'b141', 'b142', 'b143', 'b144', 'b145', 'b146', 'b147', 'b148', 'b149', 'b150', 'b151', 'b152', 'b153', 'b154', 'b155', 'b156', 'b157', 'b158', 'b159', 'b160', 'b161', 'b162', 'b163', 'b164', 'b165', 'b166', 'b167', 'b168', 'b169', 'b170', 'b171', 'b172', 'b173', 'b174', 'b175', 'b176', 'b177', 'b178', 'b179', 'b180', 'b181', 'b182', 'b183', 'b184', 'b185', 'b186', 'b187', 'b188', 'b189', 'b190', 'b191', 'b192', 'b193', 'b194', 'b195', 'b196', 'b197', 'b198', 'b199', 'b200', 'b201', 'b202', 'b203', 'b204', 'b205', 'b206', 'b207', 'b208', 'b209', 'b210', 'b211', 'b212', 'b213', 'b214', 'b215', 'b216', 'b217', 'b218', 'b219', 'b220', 'b221', 'b222', 'b223', 'b224', 'b225', 'b226', 'b227', 'b228', 'b229', 'b230', 'b231', 'b232', 'b233', 'b234', 'b235', 'b236', 'b237', 'b238', 'b239', 'b240', 'b241', 'b242', 'b243', 'b244', 'b245', 'b246', 'b247', 'b248', 'b249', 'b250', 'b251', 'b252', 'b253', 'b254', 'b255', 'b256', 'b257', 'b258', 'b259', 'b260', 'b261', 'b262', 'b263', 'b264', 'b265', 'b266', 'b267', 'b268', 'b269', 'b270', 'b271', 'b272', 'b273', 'b274', 'b275', 'b276', 'b277', 'b278', 'b279', 'b280', 'b281', 'b282', 'b283', 'b284', 'b285', 'b286', 'b287', 'b288', 'b289', 'b290', 'b291', 'b292', 'b293', 'b294', 'b295', 'b296', 'b297', 'b298', 'b299', 'b300', 'b301', 'b302', 'b303', 'b304', 'b305', 'b306', 'b307', 'b308', 'b309', 'b310', 'b311', 'b312', 'b313', 'b314', 'b315', 'b316', 'b317', 'b318', 'b319', 'b320', 'b321', 'b322', 'b323', 'b324', 'b325', 'b326', 'b327', 'b328', 'b329', 'b330', 'b331', 'b332', 'b333', 'b334', 'b335', 'b336', 'b337', 'b338', 'b339', 'b340', 'b341', 'b342', 'b343', 'b344', 'b345', 'b346', 'b347', 'b348', 'b349', 'b350', 'b351', 'b352', 'b353', 'b354', 'b355', 'b356', 'b357', 'b358', 'b359', 'b360', 'b361', 'b362', 'b363', 'b364', 'b365', 'b366', 'b367', 'b368', 'b369', 'b370', 'b371', 'b372', 'b373', 'b374', 'b375', 'b376', 'b377', 'b378', 'b379', 'b380', 'b381', 'b382', 'b383', 'b384', 'b385', 'b386', 'b387', 'b388', 'b389', 'b390', 'b391', 'b392', 'b393', 'b394', 'b395', 'b396', 'b397', 'b398', 'b399', 'b400', 'b401', 'b402', 'b403', 'b404', 'b405', 'b406', 'b407', 'b408', 'b409', 'b410', 'b411', 'b412', 'b413', 'b414', 'b415', 'b416', 'b417', 'b418', 'b419', 'b420', 'b421', 'b422', 'b423', 'b424', 'b425', 'b426', 'b427', 'b428', 'b429', 'b430', 'b431', 'b432', 'b433', 'b434', 'b435', 'b436', 'b437', 'b438', 'b439', 'b440', 'b441', 'b442', 'b443', 'b444', 'b445', 'b446', 'b447', 'b448', 'b449', 'b450', 'b451', 'b452', 'b453', 'b454', 'b455', 'b456', 'b457', 'b458', 'b459', 'b460', 'b461', 'b462', 'b463', 'b464', 'b465', 'b466', 'b467', 'b468', 'b469', 'b470', 'b471', 'b472', 'b473', 'b474', 'b475', 'b476', 'b477', 'b478', 'b479', 'b480', 'b481', 'b482', 'b483', 'b484', 'b485', 'b486', 'b487', 'b488', 'b489', 'b490', 'b491', 'b492', 'b493', 'b494', 'b495', 'b496', 'b497', 'b498', 'b499', 'b500', 'b501', 'b502', 'b503', 'b504', 'b505', 'b506', 'b507', 'b508', 'b509', 'b510', 'b511', 'b512', 'b513', 'b514', 'b515', 'b516', 'b517', 'b518', 'b519', 'b520', 'b521', 'b522', 'b523', 'b524', 'b525', 'b526', 'b527', 'b528', 'b529', 'b530', 'b531', 'b532', 'b533', 'b534', 'b535', 'b536', 'b537', 'b538', 'b539', 'b540', 'b541', 'b542', 'b543', 'b544', 'b545', 'b546', 'b547', 'b548', 'b549', 'b550', 'c50', 'c51', 'c52', 'c53', 'c54', 'c55', 'c56', 'c57', 'c58', 'c59', 'c60', 'c61', 'c62', 'c63', 'c64', 'c65', 'c66', 'c67', 'c68', 'c69', 'c70', 'c71', 'c72', 'c73', 'c74', 'c75', 'c76', 'c77', 'c78', 'c79', 'c80', 'c81', 'c82', 'c83', 'c84', 'c85', 'c86', 'c87', 'c88', 'c89', 'c90', 'c91', 'c92', 'c93', 'c94', 'c95', 'c96', 'c97', 'c98', 'c99', 'c100', 'c101', 'c102', 'c103', 'c104', 'c105', 'c106', 'c107', 'c108', 'c109', 'c110', 'c111', 'c112', 'c113', 'c114', 'c115', 'c116', 'c117', 'c118', 'c119', 'c120', 'c121', 'c122', 'c123', 'c124', 'c125', 'c126', 'c127', 'c128', 'c129', 'c130', 'c131', 'c132', 'c133', 'c134', 'c135', 'c136', 'c137', 'c138', 'c139', 'c140', 'c141', 'c142', 'c143', 'c144', 'c145', 'c146', 'c147', 'c148', 'c149', 'c150', 'c151', 'c152', 'c153', 'c154', 'c155', 'c156', 'c157', 'c158', 'c159', 'c160', 'c161', 'c162', 'c163', 'c164', 'c165', 'c166', 'c167', 'c168', 'c169', 'c170', 'c171', 'c172', 'c173', 'c174', 'c175', 'c176', 'c177', 'c178', 'c179', 'c180', 'c181', 'c182', 'c183', 'c184', 'c185', 'c186', 'c187', 'c188', 'c189', 'c190', 'c191', 'c192', 'c193', 'c194', 'c195', 'c196', 'c197', 'c198', 'c199', 'c200', 'c201', 'c202', 'c203', 'c204', 'c205', 'c206', 'c207', 'c208', 'c209', 'c210', 'c211', 'c212', 'c213', 'c214', 'c215', 'c216', 'c217', 'c218', 'c219', 'c220', 'c221', 'c222', 'c223', 'c224', 'c225', 'c226', 'c227', 'c228', 'c229', 'c230', 'c231', 'c232', 'c233', 'c234', 'c235', 'c236', 'c237', 'c238', 'c239', 'c240', 'c241', 'c242', 'c243', 'c244', 'c245', 'c246', 'c247', 'c248', 'c249', 'c250', 'c251', 'c252', 'c253', 'c254', 'c255', 'c256', 'c257', 'c258', 'c259', 'c260', 'c261', 'c262', 'c263', 'c264', 'c265', 'c266', 'c267', 'c268', 'c269', 'c270', 'c271', 'c272', 'c273', 'c274', 'c275', 'c276', 'c277', 'c278', 'c279', 'c280', 'c281', 'c282', 'c283', 'c284', 'c285', 'c286', 'c287', 'c288', 'c289', 'c290', 'c291', 'c292', 'c293', 'c294', 'c295', 'c296', 'c297', 'c298', 'c299', 'c300', 'c301', 'c302', 'c303', 'c304', 'c305', 'c306', 'c307', 'c308', 'c309', 'c310', 'c311', 'c312', 'c313', 'c314', 'c315', 'c316', 'c317', 'c318', 'c319', 'c320', 'c321', 'c322', 'c323', 'c324', 'c325', 'c326', 'c327', 'c328', 'c329', 'c330', 'c331', 'c332', 'c333', 'c334', 'c335', 'c336', 'c337', 'c338', 'c339', 'c340', 'c341', 'c342', 'c343', 'c344', 'c345', 'c346', 'c347', 'c348', 'c349', 'c350', 'c351', 'c352', 'c353', 'c354', 'c355', 'c356', 'c357', 'c358', 'c359', 'c360', 'c361', 'c362', 'c363', 'c364', 'c365', 'c366', 'c367', 'c368', 'c369', 'c370', 'c371', 'c372', 'c373', 'c374', 'c375', 'c376', 'c377', 'c378', 'c379', 'c380', 'c381', 'c382', 'c383', 'c384', 'c385', 'c386', 'c387', 'c388', 'c389', 'c390', 'c391', 'c392', 'c393', 'c394', 'c395', 'c396', 'c397', 'c398', 'c399', 'c400', 'c401', 'c402', 'c403', 'c404', 'c405', 'c406', 'c407', 'c408', 'c409', 'c410', 'c411', 'c412', 'c413', 'c414', 'c415', 'c416', 'c417', 'c418', 'c419', 'c420', 'c421', 'c422', 'c423', 'c424', 'c425', 'c426', 'c427', 'c428', 'c429', 'c430', 'c431', 'c432', 'c433', 'c434', 'c435', 'c436', 'c437', 'c438', 'c439', 'c440', 'c441', 'c442', 'c443', 'c444', 'c445', 'c446', 'c447', 'c448', 'c449', 'c450', 'c451', 'c452', 'c453', 'c454', 'c455', 'c456', 'c457', 'c458', 'c459', 'c460', 'c461', 'c462', 'c463', 'c464', 'c465', 'c466', 'c467', 'c468', 'c469', 'c470', 'c471', 'c472', 'c473', 'c474', 'c475', 'c476', 'c477', 'c478', 'c479', 'c480', 'c481', 'c482', 'c483', 'c484', 'c485', 'c486', 'c487', 'c488', 'c489', 'c490', 'c491', 'c492', 'c493', 'c494', 'c495', 'c496', 'c497', 'c498', 'c499', 'c500', 'c501', 'c502', 'c503', 'c504', 'c505', 'c506', 'c507', 'c508', 'c509', 'c510', 'c511', 'c512', 'c513', 'c514', 'c515', 'c516', 'c517', 'c518', 'c519', 'c520', 'c521', 'c522', 'c523', 'c524', 'c525', 'c526', 'c527', 'c528', 'c529', 'c530', 'c531', 'c532', 'c533', 'c534', 'c535', 'c536', 'c537', 'c538', 'c539', 'c540', 'c541', 'c542', 'c543', 'c544', 'c545', 'c546', 'c547', 'c548', 'c549', 'c550']\n['a92', 'a93', 'a94', 'a95', 'a96', 'a97', 'a98', 'a99', 'a100', 'a101', 'a102', 'a103', 'a104', 'a105', 'a106', 'a107', 'a108', 'a109', 'a110', 'a111', 'a112', 'a113', 'a114', 'a115', 'a116', 'a117', 'a118', 'a119', 'a120', 'a121', 'a122', 'a123', 'a124', 'a125', 'a126', 'a127', 'a128', 'a129', 'a130', 'a131', 'a132', 'a133', 'a134', 'a135', 'a136', 'a137', 'a138', 'a139', 'a140', 'a141', 'a142', 'a143', 'a144', 'a145', 'a146', 'a147', 'a148', 'a149', 'a150', 'a151', 'a152', 'a153', 'a154', 'a155', 'a156', 'a157', 'a158', 'a159', 'a160', 'a161', 'a162', 'a163', 'a164', 'a165', 'a166', 'a167', 'a168', 'a169', 'a170', 'a171', 'a172', 'a173', 'a174', 'a175', 'a176', 'a177', 'a178', 'a179', 'a180', 'a181', 'a182', 'a183', 'a184', 'a185', 'a186', 'a187', 'a188', 'a189', 'a190', 'a191', 'a192', 'a193', 'a194', 'a195', 'a196', 'a197', 'a198', 'a199', 'a200', 'a201', 'a202', 'a203', 'a204', 'a205', 'a206', 'a207', 'a208', 'a209', 'a210', 'a211', 'a212', 'a213', 'a214', 'a215', 'a216', 'a217', 'a218', 'a219', 'a220', 'a221', 'a222', 'a223', 'a224', 'a225', 'a226', 'a227', 'a228', 'a229', 'a230', 'a231', 'a232', 'a233', 'a234', 'a235', 'a236', 'a237', 'a238', 'a239', 'a240', 'a241', 'a242', 'a243', 'a244', 'a245', 'a246', 'a247', 'a248', 'a249', 'a250', 'a251', 'a252', 'a253', 'a254', 'a255', 'a256', 'a257', 'a258', 'a259', 'a260', 'a261', 'a262', 'a263', 'a264', 'a265', 'a266', 'a267', 'a268', 'a269', 'a270', 'a271', 'a272', 'a273', 'a274', 'a275', 'a276', 'a277', 'a278', 'a279', 'a280', 'a281', 'a282', 'a283', 'a284', 'a285', 'a286', 'a287', 'a288', 'a289', 'a290', 'a291', 'a292', 'a293', 'a294', 'a295', 'a296', 'a297', 'a298', 'a299', 'a300', 'a301', 'a302', 'a303', 'a304', 'a305', 'a306', 'a307', 'a308', 'a309', 'a310', 'a311', 'a312', 'a313', 'a314', 'a315', 'a316', 'a317', 'a318', 'a319', 'a320', 'a321', 'a322', 'a323', 'a324', 'a325', 'a326', 'a327', 'a328', 'a329', 'a330', 'a331', 'a332', 'a333', 'a334', 'a335', 'a336', 'a337', 'a338', 'a339', 'a340', 'a341', 'a342', 'a343', 'a344', 'a345', 'a346', 'a347', 'a348', 'a349', 'a350', 'a351', 'a352', 'a353', 'a354', 'a355', 'a356', 'a357', 'a358', 'a359', 'a360', 'a361', 'a362', 'a363', 'a364', 'a365', 'a366', 'a367', 'a368', 'a369', 'a370', 'a371', 'a372', 'a373', 'a374', 'a375', 'a376', 'a377', 'a378', 'a379', 'a380', 'a381', 'a382', 'a383', 'a384', 'a385', 'a386', 'a387', 'a388', 'a389', 'a390', 'a391', 'a392', 'a393', 'a394', 'a395', 'a396', 'a397', 'a398', 'a399', 'a400', 'a401', 'a402', 'a403', 'a404', 'a405', 'a406', 'a407', 'a408', 'a409', 'a410', 'a411', 'a412', 'a413', 'a414', 'a415', 'a416', 'a417', 'a418', 'a419', 'a420', 'a421', 'a422', 'a423', 'a424', 'a425', 'a426', 'a427', 'a428', 'a429', 'a430', 'a431', 'a432', 'a433', 'a434', 'a435', 'a436', 'a437', 'a438', 'a439', 'a440', 'a441', 'a442', 'a443', 'a444', 'a445', 'a446', 'a447', 'a448', 'a449', 'a450', 'a451', 'a452', 'a453', 'a454', 'a455', 'a456', 'a457', 'a458', 'a459', 'a460', 'a461', 'a462', 'a463', 'a464', 'a465', 'a466', 'a467', 'a468', 'a469', 'a470', 'a471', 'a472', 'a473', 'a474', 'a475', 'a476', 'a477', 'a478', 'a479', 'a480', 'a481', 'a482', 'a483', 'a484', 'a485', 'a486', 'a487', 'a488', 'a489', 'a490', 'a491', 'a492', 'a493', 'a494', 'a495', 'a496', 'a497', 'a498', 'a499', 'a500', 'a501', 'a502', 'a503', 'a504', 'a505', 'a506', 'a507', 'a508', 'a509', 'a510', 'a511', 'a512', 'a513', 'a514', 'a515', 'a516', 'a517', 'a518', 'a519', 'a520', 'a521', 'a522', 'a523', 'a524', 'a525', 'a526', 'a527', 'a528', 'a529', 'a530', 'a531', 'a532', 'a533', 'a534', 'a535', 'a536', 'a537', 'a538', 'a539', 'a540', 'a541', 'a542', 'a543', 'a544', 'a545', 'a546', 'a547', 'a548', 'a549', 'a550', 'b92', 'b93', 'b94', 'b95', 'b96', 'b97', 'b98', 'b99', 'b100', 'b101', 'b102', 'b103', 'b104', 'b105', 'b106', 'b107', 'b108', 'b109', 'b110', 'b111', 'b112', 'b113', 'b114', 'b115', 'b116', 'b117', 'b118', 'b119', 'b120', 'b121', 'b122', 'b123', 'b124', 'b125', 'b126', 'b127', 'b128', 'b129', 'b130', 'b131', 'b132', 'b133', 'b134', 'b135', 'b136', 'b137', 'b138', 'b139', 'b140', 'b141', 'b142', 'b143', 'b144', 'b145', 'b146', 'b147', 'b148', 'b149', 'b150', 'b151', 'b152', 'b153', 'b154', 'b155', 'b156', 'b157', 'b158', 'b159', 'b160', 'b161', 'b162', 'b163', 'b164', 'b165', 'b166', 'b167', 'b168', 'b169', 'b170', 'b171', 'b172', 'b173', 'b174', 'b175', 'b176', 'b177', 'b178', 'b179', 'b180', 'b181', 'b182', 'b183', 'b184', 'b185', 'b186', 'b187', 'b188', 'b189', 'b190', 'b191', 'b192', 'b193', 'b194', 'b195', 'b196', 'b197', 'b198', 'b199', 'b200', 'b201', 'b202', 'b203', 'b204', 'b205', 'b206', 'b207', 'b208', 'b209', 'b210', 'b211', 'b212', 'b213', 'b214', 'b215', 'b216', 'b217', 'b218', 'b219', 'b220', 'b221', 'b222', 'b223', 'b224', 'b225', 'b226', 'b227', 'b228', 'b229', 'b230', 'b231', 'b232', 'b233', 'b234', 'b235', 'b236', 'b237', 'b238', 'b239', 'b240', 'b241', 'b242', 'b243', 'b244', 'b245', 'b246', 'b247', 'b248', 'b249', 'b250', 'b251', 'b252', 'b253', 'b254', 'b255', 'b256', 'b257', 'b258', 'b259', 'b260', 'b261', 'b262', 'b263', 'b264', 'b265', 'b266', 'b267', 'b268', 'b269', 'b270', 'b271', 'b272', 'b273', 'b274', 'b275', 'b276', 'b277', 'b278', 'b279', 'b280', 'b281', 'b282', 'b283', 'b284', 'b285', 'b286', 'b287', 'b288', 'b289', 'b290', 'b291', 'b292', 'b293', 'b294', 'b295', 'b296', 'b297', 'b298', 'b299', 'b300', 'b301', 'b302', 'b303', 'b304', 'b305', 'b306', 'b307', 'b308', 'b309', 'b310', 'b311', 'b312', 'b313', 'b314', 'b315', 'b316', 'b317', 'b318', 'b319', 'b320', 'b321', 'b322', 'b323', 'b324', 'b325', 'b326', 'b327', 'b328', 'b329', 'b330', 'b331', 'b332', 'b333', 'b334', 'b335', 'b336', 'b337', 'b338', 'b339', 'b340', 'b341', 'b342', 'b343', 'b344', 'b345', 'b346', 'b347', 'b348', 'b349', 'b350', 'b351', 'b352', 'b353', 'b354', 'b355', 'b356', 'b357', 'b358', 'b359', 'b360', 'b361', 'b362', 'b363', 'b364', 'b365', 'b366', 'b367', 'b368', 'b369', 'b370', 'b371', 'b372', 'b373', 'b374', 'b375', 'b376', 'b377', 'b378', 'b379', 'b380', 'b381', 'b382', 'b383', 'b384', 'b385', 'b386', 'b387', 'b388', 'b389', 'b390', 'b391', 'b392', 'b393', 'b394', 'b395', 'b396', 'b397', 'b398', 'b399', 'b400', 'b401', 'b402', 'b403', 'b404', 'b405', 'b406', 'b407', 'b408', 'b409', 'b410', 'b411', 'b412', 'b413', 'b414', 'b415', 'b416', 'b417', 'b418', 'b419', 'b420', 'b421', 'b422', 'b423', 'b424', 'b425', 'b426', 'b427', 'b428', 'b429', 'b430', 'b431', 'b432', 'b433', 'b434', 'b435', 'b436', 'b437', 'b438', 'b439', 'b440', 'b441', 'b442', 'b443', 'b444', 'b445', 'b446', 'b447', 'b448', 'b449', 'b450', 'b451', 'b452', 'b453', 'b454', 'b455', 'b456', 'b457', 'b458', 'b459', 'b460', 'b461', 'b462', 'b463', 'b464', 'b465', 'b466', 'b467', 'b468', 'b469', 'b470', 'b471', 'b472', 'b473', 'b474', 'b475', 'b476', 'b477', 'b478', 'b479', 'b480', 'b481', 'b482', 'b483', 'b484', 'b485', 'b486', 'b487', 'b488', 'b489', 'b490', 'b491', 'b492', 'b493', 'b494', 'b495', 'b496', 'b497', 'b498', 'b499', 'b500', 'b501', 'b502', 'b503', 'b504', 'b505', 'b506', 'b507', 'b508', 'b509', 'b510', 'b511', 'b512', 'b513', 'b514', 'b515', 'b516', 'b517', 'b518', 'b519', 'b520', 'b521', 'b522', 'b523', 'b524', 'b525', 'b526', 'b527', 'b528', 'b529', 'b530', 'b531', 'b532', 'b533', 'b534', 'b535', 'b536', 'b537', 'b538', 'b539', 'b540', 'b541', 'b542', 'b543', 'b544', 'b545', 'b546', 'b547', 'b548', 'b549', 'b550', 'c92', 'c93', 'c94', 'c95', 'c96', 'c97', 'c98', 'c99', 'c100', 'c101', 'c102', 'c103', 'c104', 'c105', 'c106', 'c107', 'c108', 'c109', 'c110', 'c111', 'c112', 'c113', 'c114', 'c115', 'c116', 'c117', 'c118', 'c119', 'c120', 'c121', 'c122', 'c123', 'c124', 'c125', 'c126', 'c127', 'c128', 'c129', 'c130', 'c131', 'c132', 'c133', 'c134', 'c135', 'c136', 'c137', 'c138', 'c139', 'c140', 'c141', 'c142', 'c143', 'c144', 'c145', 'c146', 'c147', 'c148', 'c149', 'c150', 'c151', 'c152', 'c153', 'c154', 'c155', 'c156', 'c157', 'c158', 'c159', 'c160', 'c161', 'c162', 'c163', 'c164', 'c165', 'c166', 'c167', 'c168', 'c169', 'c170', 'c171', 'c172', 'c173', 'c174', 'c175', 'c176', 'c177', 'c178', 'c179', 'c180', 'c181', 'c182', 'c183', 'c184', 'c185', 'c186', 'c187', 'c188', 'c189', 'c190', 'c191', 'c192', 'c193', 'c194', 'c195', 'c196', 'c197', 'c198', 'c199', 'c200', 'c201', 'c202', 'c203', 'c204', 'c205', 'c206', 'c207', 'c208', 'c209', 'c210', 'c211', 'c212', 'c213', 'c214', 'c215', 'c216', 'c217', 'c218', 'c219', 'c220', 'c221', 'c222', 'c223', 'c224', 'c225', 'c226', 'c227', 'c228', 'c229', 'c230', 'c231', 'c232', 'c233', 'c234', 'c235', 'c236', 'c237', 'c238', 'c239', 'c240', 'c241', 'c242', 'c243', 'c244', 'c245', 'c246', 'c247', 'c248', 'c249', 'c250', 'c251', 'c252', 'c253', 'c254', 'c255', 'c256', 'c257', 'c258', 'c259', 'c260', 'c261', 'c262', 'c263', 'c264', 'c265', 'c266', 'c267', 'c268', 'c269', 'c270', 'c271', 'c272', 'c273', 'c274', 'c275', 'c276', 'c277', 'c278', 'c279', 'c280', 'c281', 'c282', 'c283', 'c284', 'c285', 'c286', 'c287', 'c288', 'c289', 'c290', 'c291', 'c292', 'c293', 'c294', 'c295', 'c296', 'c297', 'c298', 'c299', 'c300', 'c301', 'c302', 'c303', 'c304', 'c305', 'c306', 'c307', 'c308', 'c309', 'c310', 'c311', 'c312', 'c313', 'c314', 'c315', 'c316', 'c317', 'c318', 'c319', 'c320', 'c321', 'c322', 'c323', 'c324', 'c325', 'c326', 'c327', 'c328', 'c329', 'c330', 'c331', 'c332', 'c333', 'c334', 'c335', 'c336', 'c337', 'c338', 'c339', 'c340', 'c341', 'c342', 'c343', 'c344', 'c345', 'c346', 'c347', 'c348', 'c349', 'c350', 'c351', 'c352', 'c353', 'c354', 'c355', 'c356', 'c357', 'c358', 'c359', 'c360', 'c361', 'c362', 'c363', 'c364', 'c365', 'c366', 'c367', 'c368', 'c369', 'c370', 'c371', 'c372', 'c373', 'c374', 'c375', 'c376', 'c377', 'c378', 'c379', 'c380', 'c381', 'c382', 'c383', 'c384', 'c385', 'c386', 'c387', 'c388', 'c389', 'c390', 'c391', 'c392', 'c393', 'c394', 'c395', 'c396', 'c397', 'c398', 'c399', 'c400', 'c401', 'c402', 'c403', 'c404', 'c405', 'c406', 'c407', 'c408', 'c409', 'c410', 'c411', 'c412', 'c413', 'c414', 'c415', 'c416', 'c417', 'c418', 'c419', 'c420', 'c421', 'c422', 'c423', 'c424', 'c425', 'c426', 'c427', 'c428', 'c429', 'c430', 'c431', 'c432', 'c433', 'c434', 'c435', 'c436', 'c437', 'c438', 'c439', 'c440', 'c441', 'c442', 'c443', 'c444', 'c445', 'c446', 'c447', 'c448', 'c449', 'c450', 'c451', 'c452', 'c453', 'c454', 'c455', 'c456', 'c457', 'c458', 'c459', 'c460', 'c461', 'c462', 'c463', 'c464', 'c465', 'c466', 'c467', 'c468', 'c469', 'c470', 'c471', 'c472', 'c473', 'c474', 'c475', 'c476', 'c477', 'c478', 'c479', 'c480', 'c481', 'c482', 'c483', 'c484', 'c485', 'c486', 'c487', 'c488', 'c489', 'c490', 'c491', 'c492', 'c493', 'c494', 'c495', 'c496', 'c497', 'c498', 'c499', 'c500', 'c501', 'c502', 'c503', 'c504', 'c505', 'c506', 'c507', 'c508', 'c509', 'c510', 'c511', 'c512', 'c513', 'c514', 'c515', 'c516', 'c517', 'c518', 'c519', 'c520', 'c521', 'c522', 'c523', 'c524', 'c525', 'c526', 'c527', 'c528', 'c529', 'c530', 'c531', 'c532', 'c533', 'c534', 'c535', 'c536', 'c537', 'c538', 'c539', 'c540', 'c541', 'c542', 'c543', 'c544', 'c545', 'c546', 'c547', 'c548', 'c549', 'c550']\n"
     ]
    }
   ],
   "source": [
    "for file in [filename_source, filename_target]:\n",
    "# for file in [filename_source]:\n",
    "\n",
    "    # open output files in subdirectory of input files directory (must create manually)\n",
    "    fs = open(os.path.join(output_directory, file), 'r')\n",
    "\n",
    "    # read the source and target input files line by line, stripping all whitespace and empty lines\n",
    "    source_data = fs.read().strip().split('\\n')\n",
    "\n",
    "    # set min and max initial values\n",
    "    source_data_min = float('Inf')\n",
    "    source_data_max = 0.0\n",
    "\n",
    "    for i in range(len(source_data)):\n",
    "        # print([x for x in source_data[i].split(' ')])\n",
    "        source_array = np.array([int(x[1:]) for x in source_data[i].split(' ')])\n",
    "        if source_array.max() > source_data_max:\n",
    "            source_data_max = source_array.max()\n",
    "        if np.min(source_array[np.nonzero(source_array)]) < source_data_min:\n",
    "            source_data_min = np.min(source_array[np.nonzero(source_array)])\n",
    "\n",
    "    # print range of integers from min to max found in files\n",
    "    range_size = (source_data_max - source_data_min) + 1\n",
    "    samples = np.linspace(source_data_min, source_data_max, num=range_size, endpoint=True, retstep=False, dtype=int)\n",
    "    # print(samples)\n",
    "\n",
    "    samples_prefixed = []\n",
    "    prefixes = ['a', 'b', 'c']\n",
    "    [[samples_prefixed.append(prefix + str(x)) for x in samples] for prefix in prefixes]\n",
    "    print(samples_prefixed)\n",
    "\n",
    "    # save vocabulary input files to train_dir\n",
    "    filename_noext, _ = os.path.splitext(file)\n",
    "    np.savetxt(os.path.join(train_dir, filename_noext + '_vocab_input.txt'), samples_prefixed, delimiter=' ', fmt='%s')\n",
    "\n",
    "# I can't limit the vocab as I need one embedding for each value in the source and the target files. Doing a linspace\n",
    "#  creates additional values, but doesn't eliminate the existing ones.  \n",
    "# for file in [filename_target]:\n",
    "# \n",
    "#     # set min and max f0 values for target vocab (output contours)\n",
    "#     source_data_min = 50\n",
    "#     source_data_max = 550\n",
    "# \n",
    "#     # print range of integers from min to max found in files\n",
    "#     range_size = (source_data_max - source_data_min) + 1\n",
    "#     samples = np.linspace(source_data_min, source_data_max, num=range_size, endpoint=True, retstep=False, dtype=int)\n",
    "#     print(samples)\n",
    "# \n",
    "#     # save vocabulary input files to train_dir\n",
    "#     filename_noext, _ = os.path.splitext(file)\n",
    "#     np.savetxt(os.path.join(train_dir, filename_noext + '_vocab_input.txt'), samples, delimiter=' ', fmt='%u')\n",
    "\n",
    "# delete the input source and target files\n",
    "# os.remove(os.path.join(output_directory, filename_source))\n",
    "# os.remove(os.path.join(output_directory, filename_target))\n",
    "\n",
    "# now run the vocabulary script to make the proper vocab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import os.path\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Process Multiple Files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make F0 raw csv files per syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# time step is 0.005 seconds, but the syll time resolution is 4dp, so must round\n",
    "\n",
    "# for each mat file:\n",
    "#     load mat file\n",
    "#     extract f0_raw, syll_label and syll_time into variables, and reshape\n",
    "#     for each syll_label\n",
    "#         if syll_label contains a '#' in it\n",
    "#             get syll_time start and end times\n",
    "#             divide each by 0.005 to get start and end indexes\n",
    "#             extract f0_raw range using indexes\n",
    "#             do the f0 rounding, shifting, casting as before\n",
    "#             write to file, adding syllable loop index to end of filename\n",
    "#         else\n",
    "#             skip to next syllable\n",
    "\n",
    "# must also modify combo source/target section that follows, to remove segmenting parts\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "# set number of decimal places\n",
    "dec = 0\n",
    "\n",
    "# path to input files directory\n",
    "directory_path_root = '/Users/robinson/Dropbox/anasynth/_data/emoVC/Olivia2006'\n",
    "directory = os.fsencode(directory_path_root)\n",
    "# path to output files directory\n",
    "directory_path_f0raw = '/Users/robinson/Dropbox/anasynth/_data/emoVC/Olivia2006/f0_raw_syllable'\n",
    "if not os.path.exists(directory_path_f0raw):\n",
    "    os.mkdir(directory_path_f0raw)\n",
    "    \n",
    "# list to store all syllables in all files\n",
    "all_syllables = []\n",
    "\n",
    "# for each mat file in directory (each mat file has one sequence of f0 raw values in it)\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.mat'): \n",
    "        # build filepath (should use file var here really)\n",
    "        filepath = os.path.join(directory_path_root, filename)\n",
    "        # print(filepath)\n",
    "        \n",
    "        # load the file and extract f0 raw, syll_label and syll_time into variables, and reshape\n",
    "        mat_dict = loadmat(filepath)\n",
    "        f0_raw = mat_dict['f0_raw']\n",
    "        f0_raw = f0_raw.reshape((f0_raw.shape[1],))\n",
    "        \n",
    "        syll_label = mat_dict['syll_label']\n",
    "        syll_label = syll_label.reshape((syll_label.shape[1],))        \n",
    "#         print(syll_label.shape)\n",
    "        \n",
    "#         for label in syll_label:\n",
    "#             print(label[0])\n",
    "        \n",
    "        # reshape this to 2d, to preserve start/end relationship\n",
    "        syll_time = mat_dict['syll_time']\n",
    "        syll_time = syll_time.reshape((syll_time.shape[1],syll_time.shape[0]))    \n",
    "#         print(syll_time.shape)\n",
    "#         print(syll_time)\n",
    "\n",
    "#         break #debug\n",
    "        \n",
    "        # for each syll in syll_label\n",
    "        for i, syll in enumerate(syll_label):\n",
    "            \n",
    "            # if syll_label doesn't contain a '#' in it\n",
    "            if '#' not in syll[0]:\n",
    "                                \n",
    "                # add syllable to a list\n",
    "                all_syllables.append(syll[0])\n",
    "                \n",
    "                # get syll_time start and end times\n",
    "                syll_start_time = syll_time[i,0]\n",
    "                syll_end_time = syll_time[i,1]\n",
    "                \n",
    "#                 print(syll_start_time)\n",
    "#                 print(syll_end_time)                \n",
    "                \n",
    "                # divide each by 0.005 to get start and end indexes\n",
    "                syll_start_idx = (int)(syll_start_time // 0.005)\n",
    "                syll_end_idx = (int)(syll_end_time // 0.005)\n",
    "                \n",
    "#                 print(syll_start_idx)\n",
    "#                 print(syll_end_idx)\n",
    "                \n",
    "                # extract f0_raw range using indexes\n",
    "                syll_f0 = f0_raw[syll_start_idx:syll_end_idx]\n",
    "\n",
    "#                 debug\n",
    "#                 if syll[0] == 't E t':\n",
    "#                     print(filename)\n",
    "#                     print('syll_start_time ', syll_start_time)\n",
    "#                     print('syll_end_time ', syll_end_time)\n",
    "#                     print('syll_start_idx ', syll_start_idx)\n",
    "#                     print('syll_end_idx ', syll_end_idx)\n",
    "#                     print('syll_f0 ', syll_f0)\n",
    "            \n",
    "#                 print(syll_f0)\n",
    "                \n",
    "#                 break #debug\n",
    "\n",
    "                # create new array to hold rounded values\n",
    "                syll_f0_dec = np.zeros(syll_f0.shape)\n",
    "                # round all values to dec dp\n",
    "                np.around(syll_f0, decimals=dec, out=syll_f0_dec)\n",
    "                # multiply by 10^dec to shift dp dec places to the right\n",
    "                syll_f0_dec = syll_f0_dec * (10**dec)\n",
    "                # cast to int to ensure precise number representation in memory\n",
    "                syll_f0_dec = syll_f0_dec.astype(int)\n",
    "\n",
    "                # write out csv file of f0_raw values - specify format as %u for values to be written as int\n",
    "                # add syllable loop index to end of filename\n",
    "                filename_noext, _ = os.path.splitext(filename)\n",
    "                output_file_extension = '.csv'\n",
    "                output_file_name = ''.join([filename_noext, '.s', format(i, '02d'), '_', syll[0], \n",
    "                                            output_file_extension])  \n",
    "                np.savetxt(os.path.join(directory_path_f0raw, output_file_name), syll_f0_dec, delimiter=',', fmt='%u')\n",
    "            \n",
    "            # syll_label contains a '#' in it (an unvoiced region), skip to next syllable\n",
    "            else:\n",
    "                continue        \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Make Combo Source and Target Syllable Input Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# the above code makes output files with one row per syllable\n",
    "# we can't use these as-is, as we need to read the lines in as pairs, so source and target must have equal num of rows\n",
    "# next step is to pair the files using the phrase and intensities in the filenames\n",
    "\n",
    "# source: 10 phrases of i00 intensity across e01 to e08 - each phrase is said 8 times, neutrally\n",
    "# target: 10 phrases of i01-i05 intensity for e02 - each phrase is said 5 times, expressively (5 times)\n",
    "# so for each utterance (8 of) of each 'p' source phrase (10 of), copy it 5 times, matched with i01-i05 of 'p' target\n",
    "# P(10) > E(8) > I(5)\n",
    "\n",
    "# build paths and open output files\n",
    "# path to input files directories\n",
    "# input_directory_path = '/Users/robinson/Dropbox/anasynth/_data/emoVC/Olivia2006/f0_raw'\n",
    "input_directory_path = '/Users/robinson/Dropbox/anasynth/_data/emoVC/Olivia2006/f0_raw_syllable'\n",
    "# define filename components\n",
    "# Olivia2006.e02.p01.i01.csv\n",
    "input_file_root = 'Olivia2006'\n",
    "input_file_extension = '.csv'\n",
    "\n",
    "# define output filenames and paths\n",
    "output_directory = os.path.join(input_directory_path, 'out')\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "# output filenames\n",
    "filename_source = 'source.txt'\n",
    "filename_target = 'target.txt'\n",
    "filename_log = 'log.txt'\n",
    "# open output files in subdirectory of input files directory (must create manually)\n",
    "fs = open(os.path.join(output_directory, filename_source), 'w')\n",
    "ft = open(os.path.join(output_directory, filename_target), 'w')\n",
    "fo = open(os.path.join(output_directory, filename_log), 'w')\n",
    "\n",
    "\n",
    "# pass it a symbol string 'p' / 'e' / 'i' with range, or a syllable code string\n",
    "# it finds all files in a directory that have this in their filename, and returns their filenames as a set\n",
    "def getSet(symbol, num_from=None, num_to=None):\n",
    "    # path to input files directory\n",
    "    directory = os.fsencode(input_directory_path)\n",
    "    \n",
    "    # filepath_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    # for each csv file in directory (each csv file has one sequence of f0 raw values in it)\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith('.csv'): \n",
    "            # build filepath (should use file var here really)\n",
    "            # filepath = os.path.join(input_directory_path, filename)\n",
    "            \n",
    "            # if num_from is set, then it's either a p/e/i, so loop the range specified\n",
    "            if num_from != None:\n",
    "                for i in range(num_from, num_to + 1):\n",
    "                    if ''.join(['.', symbol, format(i, '02d')]) in filename:\n",
    "                        filename_list.append(filename)\n",
    "            # if num_from is not set, then it's a syllable symbol specified\n",
    "            else:\n",
    "                if ''.join(['_', symbol, '.']) in filename:\n",
    "                        filename_list.append(filename)\n",
    "\n",
    "    # return a set of unique filenames that satisfy the given parameters\n",
    "    return set(filename_list)\n",
    "\n",
    "\n",
    "# #####################\n",
    "# DEFINE PARAMETERS\n",
    "\n",
    "# define phrase range\n",
    "phrase_from = 1\n",
    "phrase_to = 10\n",
    "# define source and target emotion ranges\n",
    "source_emotion_from = 1\n",
    "source_emotion_to = 8\n",
    "target_emotion_from = 2\n",
    "target_emotion_to = 2\n",
    "# define source and target intensity ranges\n",
    "source_intensity_from = 0\n",
    "source_intensity_to = 0\n",
    "target_intensity_from = 1\n",
    "target_intensity_to = 2\n",
    "\n",
    "# END PARAMETERS\n",
    "# #######################\n",
    "\n",
    "\n",
    "# SOURCE\n",
    "# create lists of sets for each phrase, emotion, intensity and syllable code\n",
    "set_phrases = getSet('p', phrase_from, phrase_to)\n",
    "set_source_emotions = getSet('e', source_emotion_from, source_emotion_to)\n",
    "set_target_emotions = getSet('e', target_emotion_from, target_emotion_to)\n",
    "set_source_intensities = getSet('i', source_intensity_from, source_intensity_to)\n",
    "set_target_intensities = getSet('i', target_intensity_from, target_intensity_to)\n",
    "\n",
    "# print(set_source_intensities)\n",
    "\n",
    "# that do too, then I just  - do this for all syllablesfor each syllable, get set of source filenames \n",
    "# which satisfy the parameters, and a set of target filenames that do too, then I just make a set of filename pairs \n",
    "# with a loop (for each filename in source set, match with a filename in target set) - do this for all syllables\n",
    "\n",
    "# get unique list of syllables\n",
    "all_syllables_set = set([x for x in all_syllables])\n",
    "# print(len(set_one_syllable))\n",
    "# print(all_syllables_set)\n",
    "\n",
    "# for each syllable\n",
    "for syll in all_syllables_set:\n",
    "    set_one_syllable = getSet(syll)\n",
    "#     print(set_one_syllable)\n",
    "\n",
    "    # get a set of source filenames which satisfy the parameters - note: returned set can be empty\n",
    "    if set_one_syllable & set_phrases & set_source_emotions & set_source_intensities:\n",
    "        set_sources = set.intersection(set_one_syllable, set_phrases, set_source_emotions, set_source_intensities)\n",
    "#         print(set_sources)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # get a set of target filenames which satisfy the parameters\n",
    "    if set_one_syllable & set_phrases & set_target_emotions & set_target_intensities:\n",
    "        set_targets = set.intersection(set_one_syllable, set_phrases, set_target_emotions, set_target_intensities)\n",
    "#         print(set_targets)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # make a set of filename pairs (for every filename in source set, match with every filename in target set)\n",
    "    for source_file in set_sources:\n",
    "        for target_file in set_targets:\n",
    "            \n",
    "            # build the source file path\n",
    "            source_file_path = os.path.join(input_directory_path, source_file)           \n",
    "            # build the target file path\n",
    "            target_file_path = os.path.join(input_directory_path, target_file)\n",
    "            # if this file doesn't exist, break out of syllable loop and try next one\n",
    "            if not os.path.isfile(target_file_path) or os.stat(target_file_path).st_size== 0 or not os.path.isfile(source_file_path) or os.stat(source_file_path).st_size == 0:\n",
    "                break\n",
    "                \n",
    "\n",
    "            # load the source file and extract vars\n",
    "            source_f0_raw = np.loadtxt(source_file_path, dtype='int')\n",
    "            # reshape to have two indices, the first being a constant so all values belong to the same 'row'\n",
    "            source_f0_raw = source_f0_raw.reshape((1, source_f0_raw.shape[0]))                    \n",
    "            # append it to output file as a new row, with space delimiter between elements, format unsigned int\n",
    "            np.savetxt(fs, source_f0_raw, delimiter=' ', fmt='%u')\n",
    "\n",
    "            # load the target file and extract vars\n",
    "            target_f0_raw = np.loadtxt(target_file_path, dtype='int')\n",
    "            # reshape to have two indices, the first being a constant so all values belong to the same 'row'\n",
    "            target_f0_raw = target_f0_raw.reshape((1, target_f0_raw.shape[0]))                    \n",
    "            # append it to output file as a new row, with space delimiter between elements, format unsigned int\n",
    "            np.savetxt(ft, target_f0_raw, delimiter=' ', fmt='%u')\n",
    "            \n",
    "\n",
    "            # write input and output file pair to log file\n",
    "            logstring = source_file_path + '   ' + target_file_path\n",
    "            print(logstring, file=fo)\n",
    "\n",
    "# close the output files\n",
    "fs.close()\n",
    "ft.close()\n",
    "fo.close()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592\n",
      "1592\n",
      "1592\n",
      "1592\n",
      "fs_lines = 2654\n",
      "train_lines = 1592\n",
      "val_lines = 530\n",
      "test_lines = 532\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# shuffle the source/target pairs and split them out into train/val/test files\n",
    "\n",
    "# set ratios for train/val/test split e.g. 0.6, 0.2, 0.2\n",
    "train_split = 0.6\n",
    "val_split = 0.2\n",
    "test_split = 0.2\n",
    "\n",
    "# open source and target input files to read from\n",
    "fs = open(os.path.join(output_directory, filename_source), 'r')\n",
    "ft = open(os.path.join(output_directory, filename_target), 'r')\n",
    "\n",
    "# get line counts of files (source and target will be the same, so just need to check one of them)\n",
    "with open(os.path.join(output_directory, filename_source)) as f:\n",
    "    f_lines = sum(1 for _ in f)\n",
    "    # set index values for train, val and test\n",
    "    train_lines = int(f_lines // (1 / train_split))\n",
    "    val_lines = int(f_lines // (1 / val_split))\n",
    "    test_lines = f_lines - train_lines - val_lines # whatever is left\n",
    "\n",
    "# double check that source and target have the same number of lines\n",
    "with open(os.path.join(output_directory, filename_target)) as f2:\n",
    "    f_lines2 = sum(1 for _ in f2)\n",
    "    if f_lines != f_lines2:\n",
    "        raise ValueError('Not the same')\n",
    "    \n",
    "# open source and target input files to read from\n",
    "fs = open(os.path.join(output_directory, filename_source), 'r')\n",
    "ft = open(os.path.join(output_directory, filename_target), 'r')\n",
    "\n",
    "# read the source and target input files line by line, stripping all whitespace and empty lines\n",
    "source_data = fs.read().strip().split('\\n')\n",
    "# print(type(source_data))\n",
    "# print(len(source_data)) #6597\n",
    "target_data = ft.read().strip().split('\\n')\n",
    "# print(len(target_data)) #6597\n",
    "\n",
    "# make a list of tuples, each holding a pair of source and target strings\n",
    "merged_data = list(zip(source_data, target_data))\n",
    "# shuffle the tuples (preserving the pairing) to ensure a good mix of p/e/i in each set\n",
    "random.shuffle(merged_data)\n",
    "# print(len(merged_data)) #6597\n",
    "\n",
    "# seperate the tuples into two lists of source and target lines\n",
    "train_data_source = [x[0] for x in merged_data[:train_lines]]\n",
    "train_data_target = [x[1] for x in merged_data[:train_lines]]\n",
    "val_data_source = [x[0] for x in merged_data[train_lines:(train_lines+val_lines)]]\n",
    "val_data_target = [x[1] for x in merged_data[train_lines:(train_lines+val_lines)]]\n",
    "test_data_source = [x[0] for x in merged_data[(train_lines+val_lines):]]\n",
    "test_data_target = [x[1] for x in merged_data[(train_lines+val_lines):]]\n",
    "\n",
    "print(len(train_data_source))\n",
    "print(len(train_data_target))\n",
    "# print(len(val_data_source))\n",
    "# print(len(val_data_target))\n",
    "# print(len(test_data_source))\n",
    "# print(len(test_data_target))\n",
    "\n",
    "# make train, test, dev, model directories\n",
    "train_dir = os.path.join(output_directory, 'train')\n",
    "dev_dir = os.path.join(output_directory, 'dev')\n",
    "test_dir = os.path.join(output_directory, 'test')\n",
    "model_dir = os.path.join(output_directory, 'model')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "if not os.path.exists(dev_dir):\n",
    "    os.mkdir(dev_dir)\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "# open output files to write to\n",
    "f_train_source = open(os.path.join(train_dir, 'train_source.txt'), 'w')\n",
    "f_train_target = open(os.path.join(train_dir, 'train_target.txt'), 'w')\n",
    "f_val_source = open(os.path.join(dev_dir, 'val_source.txt'), 'w')\n",
    "f_val_target = open(os.path.join(dev_dir, 'val_target.txt'), 'w')\n",
    "f_test_source = open(os.path.join(test_dir, 'test_source.txt'), 'w')\n",
    "f_test_target = open(os.path.join(test_dir, 'test_target.txt'), 'w')\n",
    "\n",
    "# print(train_data_source)\n",
    "\n",
    "# write each of the lists to the opened files\n",
    "print(len([line for line in train_data_source]))\n",
    "print(len([line for line in train_data_target]))\n",
    "\n",
    "[print(line, file=f_train_source) for line in train_data_source]\n",
    "[print(line, file=f_train_target) for line in train_data_target]\n",
    "[print(line, file=f_val_source) for line in val_data_source]\n",
    "[print(line, file=f_val_target) for line in val_data_target]\n",
    "[print(line, file=f_test_source) for line in test_data_source]\n",
    "[print(line, file=f_test_target) for line in test_data_target]\n",
    "\n",
    "# close the input source and target files\n",
    "fs.close()\n",
    "ft.close()\n",
    "\n",
    "# close the output files\n",
    "f_train_source.close()\n",
    "f_train_target.close()\n",
    "f_val_source.close()\n",
    "f_val_target.close()\n",
    "f_test_source.close()\n",
    "f_test_target.close()\n",
    "\n",
    "print('fs_lines = ' + str(f_lines))\n",
    "print('train_lines = ' + str(train_lines))\n",
    "print('val_lines = ' + str(val_lines))\n",
    "print('test_lines = ' + str(test_lines))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Make Vocabulary Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175\n",
      " 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193\n",
      " 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229\n",
      " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265\n",
      " 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
      " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
      " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
      " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391\n",
      " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
      " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
      " 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445\n",
      " 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463\n",
      " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481\n",
      " 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
      " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571\n",
      " 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589\n",
      " 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607\n",
      " 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625\n",
      " 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661\n",
      " 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679\n",
      " 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715\n",
      " 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733\n",
      " 734 735 736 737 738]\n",
      "[133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150\n",
      " 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168\n",
      " 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186\n",
      " 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204\n",
      " 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222\n",
      " 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n",
      " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
      " 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276\n",
      " 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294\n",
      " 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312\n",
      " 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330\n",
      " 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348\n",
      " 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366\n",
      " 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384\n",
      " 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402\n",
      " 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420\n",
      " 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438\n",
      " 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456\n",
      " 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474\n",
      " 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492\n",
      " 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510\n",
      " 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528\n",
      " 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546\n",
      " 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564\n",
      " 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582\n",
      " 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600\n",
      " 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618\n",
      " 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636\n",
      " 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654\n",
      " 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672\n",
      " 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690\n",
      " 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706]\n"
     ]
    }
   ],
   "source": [
    "for file in [filename_source, filename_target]:\n",
    "\n",
    "    # open output files in subdirectory of input files directory (must create manually)\n",
    "    fs = open(os.path.join(output_directory, file), 'r')\n",
    "\n",
    "    # read the source and target input files line by line, stripping all whitespace and empty lines\n",
    "    source_data = fs.read().strip().split('\\n')\n",
    "\n",
    "    # set min and max initial values\n",
    "    source_data_min = float('Inf')\n",
    "    source_data_max = 0.0\n",
    "\n",
    "    for i in range(len(source_data)):\n",
    "        source_array = np.array([int(x) for x in source_data[i].split(' ')])\n",
    "        if source_array.max() > source_data_max:\n",
    "            source_data_max = source_array.max()\n",
    "        if np.min(source_array[np.nonzero(source_array)]) < source_data_min:\n",
    "            source_data_min = np.min(source_array[np.nonzero(source_array)])\n",
    "\n",
    "    # print range of integers from min to max found in files\n",
    "    range_size = (source_data_max - source_data_min) + 1\n",
    "    samples = np.linspace(source_data_min, source_data_max, num=range_size, endpoint=True, retstep=False, dtype=int)\n",
    "    print(samples)\n",
    "\n",
    "    # save vocabulary input files to train_dir\n",
    "    filename_noext, _ = os.path.splitext(file)\n",
    "    np.savetxt(os.path.join(train_dir, filename_noext + '_vocab_input.txt'), samples, delimiter=' ', fmt='%u')\n",
    "\n",
    "    \n",
    "# delete the input source and target files\n",
    "# os.remove(os.path.join(output_directory, filename_source))\n",
    "# os.remove(os.path.join(output_directory, filename_target))\n",
    "\n",
    "# now run the vocabulary script to make the proper vocab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
